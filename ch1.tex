\setvruler[][][][3][0][1.2\textwidth]

\chapter{Introduction}
\pagenumbering{arabic}
\setcounter{page}{1}

This document defines the specification of {\XMP}, a directive-based
language extension of {\Fort} and {\C} for scalable and
performance-aware parallel programming.
%
The specification includes a collection of compiler directives and
intrinsic and library procedures, and provides a model of parallel
programming for distributed memory multiprocessor systems.

%This document specifies a collection of compiler directives and runtime
%library routines that can be used to write distributed-memory parallel
%programs in {\C} and {\Fort}.These compiler directives define the
%specifications of the {\XMP} Application 
%Program Interface ({\XMP} API). These specifications provide a
%model of parallel programming for distributed memory multiprocessor
%systems. The directives extend the {\C} and {\Fort} base languages to
%describe distributed memory parallel programs.

\section{Features of {\XMP}}

The features of {\XMP} are summarized as follows:

\begin{itemize}

 \item {\XMP} supports typical parallelization based on the
       data-parallel paradigm and work mapping under ``global-view''
       programming model, and enables parallelizing the original
       sequential code using minimal modification with simple
       directives, like {\OMP} \cite{omp}. Many ideas on ``global-view''
       programming are inherited from High Performance Fortran ({\HPF})
       \cite{hpf}.

 \item The important design principle of {\XMP} is
       ``performance-awareness.'' All actions of communication and
       synchronization are taken by directives (and coarray features),
       which is different from automatic parallelizing compilers. The
       user should be aware of what happens by the {\XMP} directives in
       the execution model on the distributed memory architecture.

 \item {\XMP} also includes features from Partitioned Global Address
       Space (PGAS) languages, such as coarray of the {\Fort} 2008
       standard, for the ``local-view'' programming.

 \item Extension of existing base languages with directives is useful to
       reduce code-rewriting and education costs. The {\XMP} language
       specification is defined on {\Fort} or {\C} as a base
       language.

 \item For flexibility and extensibility, the execution model allows to
       combine with explicit Message Passing Interface ({\MPI})
       \cite{mpi} coding for more complicated and tuned parallel codes
       and libraries.

 \item For multi-core and SMP clusters, {\OMP} directives can be
       combined into {\XMP} for thread programming inside each node as a
       hybrid programming model.

% \item {\bf Language extensions} for familiar languages, such as {\C}
%       and Fortran, which can reduce code-rewriting and educational
%       costs.
%
% \item {\XMP} supports typical parallelization based on the {\bf data
%       parallel paradigm} and work sharing under {\it global view} and
%       enables parallelization of the original sequential code with
%       minimal modification using simple {\bf directives}, such as
%       {\OMP}.
%
% \item {\XMP} also includes a CAF-like Partitioned Global Address Space
%       (PGAS) feature as {\it local-view} programming.
%
% \item {\bf Explicit communication and synchronization}. All actions are
%       taken by directives for being ``easy-to-understand'' for
%       performance-aware programming
%
% \item For flexibility and extensibility, the execution model allows
%       {\bf combination with explicit {\MPI} coding} for more
%       complicated and tuned parallel codes and libraries.
%
% \item For multi-core and SMP clusters, {\bf {\OMP} directives can be
%       combined} into {\XMP} for thread programming inside each node as
%       a hybrid programming model.

\end{itemize}

{\XMP} is being designed based on experiences obtained in the
development of HPF, HPF/JA \cite{hpfja}, Fujitsu XPF (VPP
FORTRAN) \cite{XPF,VPPFORTRAN}, and OpenMPD \cite{OpenMPD}.

\section{Scope}

The {\XMP} specification covers only user-directed parallelization,
wherein the user explicitly specifies the behavior of the compiler and
the runtime system in order to execute the program in parallel in a
distributed-memory system.
%
{\XMP}-compliant implementations are not required to automatically
lay out data, detect parallelism and parallelize loops, or generate
communications and synchronizations.

%The {\XMP} is defined by following items:
%
%\begin{itemize}
%\item A set of directives
%\item Minimum language extension on base languages ({\C} and {\Fort})
%\item Runtime libraries
%\item Environment Variables
%\end{itemize}

\section{Organization of this Document}

The remainder of this document is structured as follows:

\begin{itemize}
 \item Chapter 2: Overview of the {\XMP} Model and Language
 \item Chapter 3: Directives 
 \item Chapter 4: Support for the Local-view Programming
 \item Chapter 5: Base Language Extensions in {\XMPC}
 \item Chapter 6: Procedure Interface
 \item Chapter 7: Intrinsic and Library Procedures
 \item Chapter 8: OpenMP in XcalableMP Programs
\end{itemize}
%
In addition, the following appendices are included in this document as
proposals.
%
\begin{itemize}
 \item Appendix A: Programming Interface for MPI
% \item Appendix B: Directive for Thread Parallelism
 \item Appendix B: Interface to Numerical Libraries
 \item Appendix C: Memory-layout Model
 \item Appendix D: XcalableMP I/O
\end{itemize}

\section{Changes from Version 1.3}
\begin{itemize}
  \item In XcalableMP C, a square bracket is available in {\it nodes-decl}, {\it nodes-ref}, 
  {\it template-ref}, and {\it template-decl}.
  \item Add the {\tt orthogonal} clause to the {\tt reflect} directive in Section \ref{sub:reflect}.
  \item Create {\tt xmp\_node\_rank()} in Section \ref{subsec:xmpnoderank}.
  \item Modify {\tt xmp\_array\_gtol()} in Section \ref{subsec:xmparraygtol}.
  \item Modify {\tt xmp\_array\_lead\_dim()} in Section \ref{subsec:xmparrayleaddim}.
\end{itemize}

\section{Changes from Version 1.2}

\begin{itemize}
 % \item The concept of the node array and the node set is reorganized.
 % \item Mapping inquiry procedures are expanded and moved from the
 %       appendix to the core specification.
 % \item The specification on coarrays is improved significantly and some
 %       image control directives in XMP/C are defined.
 % \item The appendix on a directive for thread parallelism is deleted.
 % \item The proposal on XcalableMP I/O is changed slightly and adopted.
 % \item etc.
 % \item The concept of built-in function is introduced into {\XMPC}.
 % \item The semantics of array intrinsic functions of the base language
 %       appearing in {\XMPF} programs is defined.
 % \item Built-in elemental functions for {\XMPF} are defined.
 % \item Intrinsic/built-in transformational procedures are defined.
 % \item The rule for mixing {\XMP} and OpenMP in a program is defined.
  \item The position of {\tt align} directives for dummy arguments
		in {\XMPC} is specified.
  \item It is specified that aligned arrays cannot be initialized.
  \item Interpretation of a {\tt reduction} clause of the {\tt loop}
		directive is corrected.
  \item The syntax for declaring coarrays is changed.
  \item An assumed-shape array can be the target of the {\tt
		local\_alias} directive.
  \item The syntax and the semantics of the array section notation in
		{\XMPC} is modified.
  \item The syntax of the array assignment statement in {\XMPC} is
		extended.
\end{itemize}


\cleardoublepage

\chapter{Overview of the {\XMP} Model and Language}
\label{chap: overview}

\section{Hardware Model}

The target of {\XMP} is distributed-memory multicomputers (Figure
\ref{fig1}). Each computation node, which may contain several cores, has
its own local memory (shared by the cores, if any), and is connected
with each other via an interconnection network.
%
Each node can access its local memory directly and remote memory, that
is, the memory of another node indirectly (i.e. via
communication). However, it is assumed that accessing remote memory is 
much slower than accessing local memory.

\begin{myfigure}
\includegraphics[width=12cm]{figs/Fig1.eps}
  \caption{Hardware Model}\label{fig1}
\end{myfigure}

\section{Execution Model}

An {\XMP} program execution is based on the Single Program Multiple Data
(SPMD) model, where each node starts execution from the same main
routine and keeps executing the same code independently
(i.e. asynchronously), which is referred to as the {\it \Term{replicated
execution}}, until it encounters an {\XMP} construct.

%The basic execution model of {\XMP} is a Single Program Multiple Data
%(SPMD) model on distributed memory. In each node, a program starts from
%the same main routine.
%
%Unless the nodes encounter some {\XMP} directives, they executes the same
%code locally (i.e. asynchronously), which is referred to as
%{\it \Term{duplicate execution}}.

%An {\XMP} program begins as a single thread of
%execution in each node. 

%In this case, the
%program performs duplicate execution of the same program on local memory
%in each node.

%{\OMP} API can be used in order to make use of multicores in a node. In
%this specification, we define actions only when {\XMP} directives are
%executed one thread at a time.

A set of nodes that executes a procedure, a statement, a loop,
a block, etc. is referred to as its {\it \Term{executing node set}} and
determined by the innermost {\tt task}, {\tt loop} or {\tt array}
directive surrounding it dynamically, or at runtime.
%
The {\it \Term{current executing node set}} is an executing node set of
the current context, which is managed by the {\XMP} runtime system on
each node.

%The initial ``current executing node set'' (or the {\it \Term{entire
%node set}}) at the beginning of the program execution is the set of all
%available nodes, which can be specified in an implementation-dependent
%way (e.g. through a command-line option).

The current executing node set at the beginning of the program
execution, or {\it \Term{primary node set}}, is a node set that
contains all the available nodes, which can be specified in an 
implementation-dependent way (e.g. through a command-line option).
%
%The primary node array is the node array if specified explicitly by the
%{\tt nodes} directive or an implicit one-dimensional node array if not.
%}

When a node encounters at runtime either a {\tt loop}, {\tt array}, or
{\tt task} construct, and is contained by the node set specified by the
{\tt on} clause of the directive, it updates the current executing node
set with the specified one and executes the body of the construct, after
which it resumes the last executing node set and proceeds to execute the
following statements.

Particularly when a node in the current executing node set encounters a
{\tt loop} or an {\tt array} construct, it executes the loop or the array
assignment in parallel with other nodes, so that each iteration of the
loop or element of the assignment is independently executed by the node
where a specified data element resides.

When a node encounters a synchronization or a communication directive,
synchronization or communication occurs between it and other nodes.
%
That is, such {\it \Term{global constructs}} are performed collectively
by the current executing nodes.
%
Note that neither synchronizations nor communications occur without these
constructs specified.


\section{Data Model}

%By default, data declared in the program are allocated in each node and
%are referenced locally by threads executed in the node. 

There are two classes of data in {\XMP}: {\it \Term{global data}} and
{\it \Term{local data}}. Data declared in an {\XMP} program are local by
default.

Global data are ones that are distributed onto the executing node set by
the {\tt align} directive (see section \ref{sub:align}). Each fragment
of a global data is allocated in the local memory of a node in the
executing node set.
%
%Note that the ``address'' of a global data is defined as that of its
%local section in each node and the results of any operations on such
%address are undefined.
%
%In contrast to a local-view programming model, a global-view programming
%model is a model in which programmers express their algorithm and data
%structure in their entirety, mapping them to the node set. The
%programmers describe the data distribution and the work mapping in order
%to express how to distribute data and share the workload among
%nodes. The variables in the global-view programming model appear as a
%shared memory spanning the nodes.

Local data are all of the ones that are not global. They are replicated
in the local memory of each of the executing nodes.

%{\XMP} supports two models of data viewing: the global-view programming
%model and the local-view programming model. In the local-view
%programming model, accesses to data in remote nodes are performed
%explicitly by language extension for get/put operations on remote nodes
%with the node number of the target nodes, while reference to local data
%is executed implicitly.

A node can access directly only local data and sections of global data
that are allocated in its local memory.
%
To access data in remote memory, explicit communication must be
specified in such ways as the global communication constructs and
the coarray assignments.

%\underline{Description on memory layout to be added.}

Particularly in {\XMPF}, for common blocks that include any global
variables, the ways how the storage sequence of them is defined and how
the storage association of them is resolved are
implementation-dependent.

\section{Global-view Programming Model}

The global-view programming model is useful when, starting from a
sequential version of a program, the programmer parallelizes it in
data-parallel style by adding directives with minimum modification.
%
In the global-view programming model, the programmer describes the
distribution of the data among nodes using the data distribution
directives.
%
The {\tt loop} construct assigns each iteration of a loop to the node
where the computed data is located. 
%
The global-view communication directives are used to synchronize nodes,
to maintain the consistency of the shadow area, and to move part of the
distributed data globally.
%
Note that the programmer must specify explicitly communications to make
all data reference in the program local by using appropriate directives.
%Note that the programmer must perform all computations that require data
%reference locally by any appropriate directives.

In many cases, the {\XMP} program according to the global-view
programming model is based on a sequential program and can produce the
same results as it, regardless of the number of nodes (Figure
\ref{fig2}).
%The global view provides a 
%programming model in which computation and data are distributed onto
%computation nodes.

There are three groups of directives for the global-view programming
model. Since these directives are ignored as a comment by the
compilers of base languages ({\Fort} and {\C}), an {\XMP} program can be
compiled by them to run properly.

%an  {\XMP} program derived from a sequential program can preserve the
%integrity of the original program when the program is run sequentially. 

\subsubsection*{Data Mapping}

Specifies the data distribution and mapping to nodes (partially
inherited from HPF).

\subsubsection*{Work Mapping (Parallelization)}

Assigns a work to a node set. The {\tt loop} construct maps each
iteration of a loop to nodes owning a specified data elements. The {\tt
task} construct defines an amount of work as a {\it \Term{task}} and
assigns it to a specified node set.

\subsubsection*{Communication and Synchronization}

Specifies how to communicate and synchronize with the other compute
nodes. In {\XMP}, inter-node communication must be explicitly specified
by the programmer. The compiler guarantees that no communication occurs
unless it is explicitly specified by the programmer.

\begin{myfigure}
\includegraphics[width=12cm]{figs/Fig2.eps}
  \caption{Parallelization by the Global-view Programming Model}
\label{fig2}
\end{myfigure}

\section{Local-view Programming Model}

The local-view programming model is suitable for programs that
explicitly describe an algorithm and remote data reference that are to
be done by each node (Figure \ref{fig3}).
%Since MPI is based on the local-view model, the local-view programming
%model of {\XMP} has high interoperability with MPI.

For the local-view programming model, some language extensions and 
directives are provided. The coarray notation imported from {\Fort} 2008
is one of such extensions and can be used to specify which replica of a
local data is to be accessed. For example, the expression of {\tt
A(i)[N]} is used to access an array element of {\tt A(i)} located on the
node {\tt N}.
%
If the access is a reference, then communication to obtain the value
from remote memory (i.e. {\it get} operation) occurs. If the access is a
definition, then communication to set a value to remote memory
(i.e. {\it put} operation) occurs.

\begin{myfigure}
\includegraphics[width=12cm]{figs/Fig3.eps}
  \caption{Local-view Programming Model}
\label{fig3}
\end{myfigure}

\section{Interactions between the Global View and the Local View}

In the global view, nodes are used to distribute data and computational
load. In the local view, nodes are used to address data in the coarray
notation.
%
In the application program,
programmers should choose an appropriate data model according to the
structure of the program. Figure \ref{fig4} illustrates the global view
and the local view of data.

Data may have both a global view and a local view, and can be accessed
from either. {\XMP} provides some directives to give the local name
(alias) to the global data declared in the global-view programming model
so that they can be accessed also in the local-view programming
model. This feature is useful to optimize a certain part of the program
by using explicit remote data access in the local-view programming
model.

\begin{myfigure}
\includegraphics[width=12cm]{figs/Fig4.eps}
  \caption{Global View and Local View}
\label{fig4}
\end{myfigure}

\section{Base Languages}

The XcalableMP language specification is defined on Fortran or C as a
base language. More specifically, the base language of XcalableMP
Fortran is Fortran 90 or later, and that of XcalableMP C is ISO C90
(ANSI C89) or later.

%\section{Execution model and task}
%
%
%In {\XMP}, a program begins as a single thread
%of execution in each node. The set of nodes when starting a program is
%referred to as the entire node set.
%
%A task is a specific instance of executable
%code and its data environment executed in a set of nodes. A task when
%starting a program in the entire node set is called an initial task. The
%initial task can generate a subtask, which is executed on a subset of the
%nodes by the {\tt task} construct. A set of nodes executing the same task is
%referred to as the set of executing nodes. If no {\tt task} construct is encountered, then a
%program is executed as a single task, and its executing nodes are the entire node set.
%
%If no directives are encountered, then a program is executed
%locally. When the same codes are executed, almost the same computation is
%performed in each node, which is referred to as duplicate execution. When the threads
%encounter a {\tt loop} construct or an {\tt array} construct, the specified
%loop is executed in parallel, so that each iteration is assigned to the
%node where the specified data element is located. 
%
%A new task is generated by
%the {\tt task} construct. A code in the {\tt task} construct is executed
%as a subtask executed in a specified node set. When a subroutine is
%called in the context of the task, the subroutine is executed on its
%executing nodes. 
%
%For synchronization and communication between nodes, a set of
%directives is provided. In the local-view programming model, coarray
%features are adopted for remote data reference. Note 
%that all synchronization and communication are specified explicitly by directives, and without such directives, no communications are
%executed implicitly by the compiler.
