\chapter{Programming Interface for {\MPI} and {\OMP}}

   This section describes the programming interface for {\MPI} and {\OMP},
   which are widely used for parallel programming for cluster computing.
   Users can introduce {\MPI} and {\OMP} functions to {\XMP} using the interface.   

\section{{\MPI} Interface}

   {\XMP} provides the following user API functions to mix {\MPI} functions with {\XMP}

\begin{itemize}
\item {\tt xmp\_get\_mpi\_comm}
\item {\tt xmp\_init\_mpi}
\item {\tt xmp\_finalize\_mpi}
\end{itemize}

\subsection{\tt xmp\_get\_mpi\_comm}

\subsubsection*{Format}

\begin{tabular}{lll}

\verb![C]!&  {\tt int}& {\tt xmp\_get\_mpi\_comm(nodes-name)}\\

\verb![F]!&  {\tt integer function}& {\tt xmp\_get\_mpi\_comm(nodes-name)}
\end{tabular}

\subsubsection*{Synopsis}
   xmp\_get\_mpi\_comm returns the integer value with the associated communicator to which
   nodes-name belongs. xmp\_mpi\_comm returns the executing MPI communicator when nodes-name is omitted.

\subsubsection*{Arguments}
   nodes-name is the name of a node set.

\subsection{\tt xmp\_init\_mpi}

\subsubsection*{Format}

\begin{tabular}{lll}

\verb![C]!&  {\tt void}& {\tt xmp\_init\_mpi(int *args, char ***argv)}\\

\verb![F]!&  {\tt }& {\tt xmp\_init\_mpi()}
\end{tabular}

\subsubsection*{Synopsis}

   xmp\_init\_mpi initializes MPI execution environment.

\subsubsection*{Arguments}

   In C, argc and argv, command-line arguments, should be given to xmp\_init\_mpi.


\subsection{\tt xmp\_finalzie\_mpi}

\subsubsection*{Format}

\begin{tabular}{lll}

\verb![C]!&  {\tt void}& {\tt xmp\_finalize\_mpi(void)}\\

\verb![F]!&  {\tt }& {\tt xmp\_finalize\_mpi()}
\end{tabular}

\subsubsection*{Synopsis}

   xmp\_finalize\_mpi terminates MPI execution enviroment.

\subsubsection*{Arguments}

   none.

\subsection*{Example}
\begin{Cexample}
#include <stdio.h>
#include "mpi.h"
#include "xmp.h"

#pragma xmp nodes p(4)

int main(int argc, char *argv[]) {
  xmp_init_mpi(&argc, &argv)

  int rank, size;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

#pragma xmp task on p(2:3)
{
  MPI_Comm comm = xmp_get_mpi_comm(); // get the MPI communicator of p(2:3)

  int rank, size;
  MPI_Comm_rank(comm, &rank);
  MPI_Comm_size(comm, &size);
}

  xmp_finalize_mpi();

  return 0;
}
\end{Cexample}


\section{{\OMP}}

   Thread-level parallelism is needed to program multi-core cluster system.
   Users can use {\OMP} functions using threads-clause in loop directive.
   Direct use of {\OMP} directives in {\XMP} code is not allowed.

\subsection*{Syntax}

{\it threads-clause} is:

\begin{tabular}{ll}
 \hspace{0.5cm} & threads {\openb} {\it omp-clause} {\closeb} \\
\end{tabular}

{\it omp-clause} is one of:

\begin{tabular}{ll}
 & {\tt num\_threads} \\
 & {\tt private} \\
 & {\tt firstprivate} \\
 & {\tt lastprivate} \\
\end{tabular}

\subsection*{Description}

   {\OMP} clauses such as num\_threads can be desctibed in threads-clause.
   The {\XMP} compiler generates {\OMP} directives from the {\XMP} loop statement and the threads-clause.
   reduction-clause does not need to be desctibed in threads-clause
   because it is inherited from reduction-clause in {\XMP} loop directive.

\subsection*{Example}

   The example calculates total sum of array a.
   threads-clause is given to the loop directive to parallelize the loop statement in both process-level and thread-level.
   The reduction-clause in the loop directive is applied to {\OMP} for directive which is generated by the {\XMP} compiler.

\begin{Cexample}
#include <stdio.h>
#include "xmp.h"
#define N 1024

#pragma xmp nodes p(*)
#pragma xmp template t(0:N-1)
#pragma xmp distribute t(block) onto p
#pragma xmp align a[i] with t(i)

int main(void) {
  . . . // initialize a[]

  int sum = 0;
#pragma xmp loop on t(i) reduction(+:sum) threads num\_threads(4)
  for (int i = 0; i < N; i++) {
    sum += a[i];
  }

  return 0;
}
\end{Cexample}


\chapter{Interface to Numerical Libraries}

   This section describe XcalableMP interfaces to existing MPI parallel library, 
   in order to develop good productive and high performance XcalableMP programs 
   using existing MPI parallel libraries.
   
\section{ScaLAPACK}

   This subsection shows main characteristics in case of developing
   XcalableMP programs using XcalableMP interfaces to ScaLAPACK library routines.

\begin{itemize}
\item The name of XcalableMP interface prefixes "XMP\_" or "xmp\_" 
      to ScaLAPACK library routine name.
\item Execution flow is the following:\\
      XMP programs $\to$ XMP interface $\to$ native ScaLAPACK library routines
\item If arguments of subroutines have array descriptor,
      we replace the argument with "xmp\_desc\_of".
      Then we don't need array descriptor. 
\item For ScaLAPACK library routine having descriptor array as argumet,
      the XcalableMP interface routine have BLACS context handle including 
      the descriptor array as new argument.
\item The blacs\_exit routine is unnecessary, because XcalableMP program executes process 
      corresponding to MPI\_Finalize routines.
\item A valid value of the argument "order" of BLACS routine "blacs\_gridinit" is 
      only column-major. 
\end{itemize}
